{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_vector_store(vector_store_dir: str):\n",
    "    \"\"\"Load the FAISS vector store from disk.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\", openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    return FAISS.load_local(vector_store_dir, embeddings, allow_dangerous_deserialization=True,)\n",
    "\n",
    "\n",
    "def create_qa_chain(vector_store):\n",
    "    \"\"\"Create a question-answering chain using GPT-4.\"\"\"\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"gpt-4.1\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.0\n",
    "    )\n",
    "\n",
    "    # Create memory for conversation history\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    # Custom prompt template\n",
    "    template = \"\"\"You are an AI assistant that helps answer questions based on the provided context. \n",
    "    Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Chat History: {chat_history}\n",
    "    Human: {question}\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"chat_history\", \"question\"], template=template\n",
    "    )\n",
    "\n",
    "    # Create the chain\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": 3}  # Retrieve top 3 most relevant chunks\n",
    "        ),\n",
    "        memory=memory,\n",
    "        combine_docs_chain_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    )\n",
    "\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(\n",
    "        \"query_documents\",\n",
    "        description=\"This tool helps to query the document\"\n",
    ") \n",
    "def query_documents(question: str):\n",
    "    \"\"\"\n",
    "    This tool helps to query the document\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question you would like to ask inorder to retrive the information\n",
    "\n",
    "    Returns:\n",
    "        list: The query results \n",
    "    \"\"\"\n",
    "    # Load the vector store\n",
    "    vector_store_dir: str = os.getenv(\"RAG_VECTORSTORE_PATH\", \"vector_store\")\n",
    "    vector_store = load_vector_store(vector_store_dir)\n",
    "\n",
    "    # Create QA chain\n",
    "    qa_chain = create_qa_chain(vector_store)\n",
    "\n",
    "    # Get response\n",
    "    response = qa_chain({\"question\": question})\n",
    "\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "BUSINESS_CONTEXT = \"\"\"\n",
    "Tesla is a manifacturing company that produces electric cars, solar panels, and energy storage systems.\n",
    "\"\"\"\n",
    "\n",
    "RAG_AGENT_PROMPT = \"\"\"\n",
    "You are a helpful assistant who can answer questions based on the provided context.\n",
    "\n",
    "You're also given a tool, \"query_documents\" to retrive the information from relavant document\n",
    "\n",
    "Take the following steps to provide the answer:\n",
    "1. write reasoning steps to approach the question.\n",
    "2. Retrive the relavant information (please feel free to go on multiple iteration until you find disired information)\n",
    "3. provide the answer to the user\n",
    "\n",
    "here is the semantic context you can use to understand the business and generate the query:\n",
    "----*****Semantic Context*****----\n",
    "{semantic_context}\n",
    "----*****END OF Semantic Context*****----\n",
    "\n",
    "Expectation:\n",
    "- if the information is not present or the result is empty, inform the to the user rather than guessing.\n",
    "- if you don't have enough information to generate the query, ask for more information.\n",
    "- please include name whenever possible instead of ids, for example instead of device id provide device name too.\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "rag_agent = create_react_agent(\n",
    "    name=\"rag_agent\",\n",
    "    model=model,\n",
    "    tools=[query_documents],\n",
    "    prompt=RAG_AGENT_PROMPT.format(semantic_context=BUSINESS_CONTEXT),\n",
    "    checkpointer=memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"02\"},    \"recursion_limit\": 20}\n",
    "messages = [HumanMessage(content=\"\"\"\n",
    "Research and development expenses during Q2-2024? \n",
    "\"\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content=\"Reasoning steps:\\n1. The user is asking for Tesla's research and development (R&D) expenses during Q2-2024.\\n2. I need to look for financial data or quarterly reports that mention Tesla's R&D expenses for Q2-2024.\\n3. I will query the documents with a specific question to retrieve this information.\\n\\nQuerying for relevant information...\", additional_kwargs={'tool_calls': [{'id': 'call_38V8mO8hptsmqbA6edCGyhKy', 'function': {'arguments': '{\"question\":\"What were Tesla\\'s research and development expenses during Q2-2024?\"}', 'name': 'query_documents'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 673, 'total_tokens': 778, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-BaTTzI38PqZufliNa05lOGIUqPxPm', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, name='rag_agent', id='run--9ba22787-9f11-4d76-b3cc-f93b47a86b73-0', tool_calls=[{'name': 'query_documents', 'args': {'question': \"What were Tesla's research and development expenses during Q2-2024?\"}, 'id': 'call_38V8mO8hptsmqbA6edCGyhKy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 673, 'output_tokens': 105, 'total_tokens': 778, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "{'messages': [ToolMessage(content=\"Tesla's research and development expenses during Q2-2024 were $1,074 million.\", name='query_documents', id='4b7905fe-307a-4fa0-ae6f-99c89c7ec635', tool_call_id='call_38V8mO8hptsmqbA6edCGyhKy')]}\n",
      "{'messages': [AIMessage(content=\"Tesla's research and development expenses during Q2-2024 were $1,074 million.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 814, 'total_tokens': 834, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-BaTU53MOJkEvr2LMbnOztDf78sDre', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='rag_agent', id='run--ffc147b4-b62f-4b90-b59e-214c4874fc16-0', usage_metadata={'input_tokens': 814, 'output_tokens': 20, 'total_tokens': 834, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "final_message = None\n",
    "for event in rag_agent.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)\n",
    "        if v['messages'][-1].content:\n",
    "            final_message = v['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tesla's research and development expenses during Q2-2024 were $1,074 million.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
